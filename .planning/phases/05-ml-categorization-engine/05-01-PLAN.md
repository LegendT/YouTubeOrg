---
phase: 05-ml-categorization-engine
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/lib/ml/embeddings-cache.ts
  - src/lib/ml/worker.ts
  - src/lib/ml/similarity.ts
  - src/lib/ml/confidence.ts
autonomous: true

must_haves:
  truths:
    - "System can generate embeddings for video titles without blocking UI"
    - "System caches embeddings in IndexedDB to avoid recomputation"
    - "System calculates cosine similarity between video and category embeddings"
    - "System assigns confidence levels (HIGH/MEDIUM/LOW) based on similarity scores"
  artifacts:
    - path: "src/lib/ml/embeddings-cache.ts"
      provides: "IndexedDB wrapper for persistent embedding storage"
      exports: ["EmbeddingsCache"]
      min_lines: 80
    - path: "src/lib/ml/worker.ts"
      provides: "Web Worker with Transformers.js singleton pipeline"
      exports: []
      min_lines: 60
    - path: "src/lib/ml/similarity.ts"
      provides: "Cosine similarity calculation for normalized vectors"
      exports: ["cosineSimilarity"]
      min_lines: 20
    - path: "src/lib/ml/confidence.ts"
      provides: "Confidence scoring with threshold-based levels"
      exports: ["getConfidenceLevel", "CONFIDENCE_THRESHOLDS"]
      min_lines: 30
  key_links:
    - from: "src/lib/ml/worker.ts"
      to: "@huggingface/transformers"
      via: "PipelineSingleton pattern"
      pattern: "pipeline\\(.*feature-extraction"
    - from: "src/lib/ml/embeddings-cache.ts"
      to: "IndexedDB"
      via: "IDBDatabase transaction"
      pattern: "transaction\\(.*embeddings"
    - from: "src/lib/ml/confidence.ts"
      to: "src/lib/ml/similarity.ts"
      via: "cosine similarity import"
      pattern: "import.*cosineSimilarity"
---

<objective>
Implement the foundational ML infrastructure for client-side video categorization: IndexedDB embeddings cache for persistent storage, Web Worker with Transformers.js singleton pipeline for non-blocking inference, and cosine similarity functions with confidence scoring.

Purpose: Establish the core ML primitives required for processing 4,000+ videos without browser freezing. These independent components form the building blocks for the categorization engine.

Output: Four TypeScript modules providing embeddings cache, Web Worker, similarity calculations, and confidence scoring. All ready for orchestration in subsequent plans.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-ml-categorization-engine/05-RESEARCH.md

# Schema context for understanding existing data structures
@src/lib/db/schema.ts

# Type context for category and video data
@src/types/categories.ts
@src/types/videos.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: IndexedDB Embeddings Cache</name>
  <files>src/lib/ml/embeddings-cache.ts</files>
  <action>
Create IndexedDB wrapper class for persistent embedding storage following the pattern from 05-RESEARCH.md Pattern 3.

**Implementation:**
- Open database named "ml-embeddings" with version 1
- Create object store "embeddings" with compound key [videoId, modelVersion]
- Index on generatedAt for cleanup queries
- EmbeddingRecord interface: { videoId: number, modelVersion: string, embedding: Float32Array, generatedAt: Date }
- EmbeddingsCache class with methods:
  - constructor(): Initialize DB connection using IDBDatabase API
  - get(videoId: number, modelVersion: string): Promise<Float32Array | null>
  - set(videoId: number, modelVersion: string, embedding: Float32Array): Promise<void>
  - getBatch(videoIds: number[], modelVersion: string): Promise<Map<number, Float32Array>>
  - setBatch(entries: Array<{ videoId: number; embedding: Float32Array }>, modelVersion: string): Promise<void>
  - clear(modelVersion?: string): Promise<void> // Optional: clear old model versions

**Key details:**
- Use compound index [videoId, modelVersion] for efficient lookups across model upgrades
- Store Float32Array directly (IndexedDB supports typed arrays)
- Batch methods use single transaction for performance (avoid deadlocks per RESEARCH.md Pitfall 3)
- Export EmbeddingsCache class as default export
- Add error handling for quota exceeded (50-100MB browser limit)

**Avoid:**
- Don't use localStorage (5MB limit insufficient for 6MB of embeddings)
- Don't open multiple transactions in parallel (causes deadlocks)
- Don't convert Float32Array to regular arrays (loses performance benefit)
  </action>
  <verify>
Run: npm run type-check (no TypeScript errors)
Check: File exists at src/lib/ml/embeddings-cache.ts with EmbeddingsCache class exported
Check: IndexedDB schema includes compound key [videoId, modelVersion]
  </verify>
  <done>
EmbeddingsCache class implemented with get/set/getBatch/setBatch methods, IndexedDB initialized with "embeddings" object store, compound key indexing configured, error handling for quota limits included.
  </done>
</task>

<task type="auto">
  <name>Task 2: Web Worker with Transformers.js Singleton</name>
  <files>src/lib/ml/worker.ts</files>
  <action>
Create Web Worker implementing singleton pattern for Transformers.js feature-extraction pipeline following 05-RESEARCH.md Pattern 1.

**First, install dependency:**
```bash
npm install @huggingface/transformers
```

**Implementation:**
- Import { pipeline, env } from '@huggingface/transformers'
- Set env.allowLocalModels = false (skip local model checks in browser per RESEARCH.md)
- PipelineSingleton class:
  - static task = 'feature-extraction'
  - static model = 'Xenova/all-MiniLM-L6-v2'
  - static instance = null
  - static async getInstance(progress_callback = null): returns cached or new pipeline
- addEventListener for 'message' events handling:
  - type: 'GENERATE_EMBEDDINGS' — batch processing
    - texts: string[] — array of video titles/descriptions
    - id: string — correlation ID for response matching
    - Call extractor with { pooling: 'mean', normalize: true }
    - Convert output tensors to Float32Array: Array.from(tensor.data)
    - postMessage with type: 'EMBEDDINGS_RESULT', id, embeddings: Float32Array[]
  - type: 'LOAD_PROGRESS' — forward progress_callback from pipeline loading
    - postMessage with type: 'LOAD_PROGRESS', progress
  - type: 'TERMINATE' — self.close()

**Key details:**
- Singleton ensures model (50-100MB) loads once per session
- normalize: true ensures embeddings are unit vectors (cosine similarity = dot product)
- pooling: 'mean' converts token embeddings to single sentence embedding
- Use transferable objects for zero-copy transfer (RESEARCH.md Pattern 5): postMessage(data, [array.buffer])
- Model downloads to browser cache (Cache API), subsequent loads are instant

**Avoid:**
- Don't reload pipeline on every message (expensive 5-10s model load)
- Don't use WebGPU by default (RESEARCH.md Pitfall 2: device-specific memory limits)
- Don't forget normalize: true (breaks cosine similarity assumptions)
  </action>
  <verify>
Run: npm run type-check (no TypeScript errors)
Check: File exists at src/lib/ml/worker.ts
Check: Code includes PipelineSingleton class with getInstance method
Check: Message handler supports GENERATE_EMBEDDINGS and LOAD_PROGRESS types
Check: @huggingface/transformers appears in package.json dependencies
  </verify>
  <done>
Web Worker implemented with PipelineSingleton pattern, Transformers.js dependency installed, message handlers for embeddings generation and progress tracking, model caching via singleton instance, transferable objects for efficient worker communication.
  </done>
</task>

<task type="auto">
  <name>Task 3: Cosine Similarity and Confidence Scoring</name>
  <files>src/lib/ml/similarity.ts, src/lib/ml/confidence.ts</files>
  <action>
Create utility functions for cosine similarity calculation and threshold-based confidence scoring following 05-RESEARCH.md Pattern 4.

**File: src/lib/ml/similarity.ts**
```typescript
/**
 * Calculates cosine similarity between two normalized vectors.
 * For normalized vectors (from Transformers.js with normalize: true),
 * cosine similarity simplifies to dot product.
 *
 * @param a - First embedding vector (Float32Array)
 * @param b - Second embedding vector (Float32Array)
 * @returns Similarity score in range [0, 1] where 1 = identical
 */
export function cosineSimilarity(a: Float32Array, b: Float32Array): number {
  if (a.length !== b.length) {
    throw new Error(`Vector dimensions must match: ${a.length} vs ${b.length}`);
  }

  let dotProduct = 0;
  for (let i = 0; i < a.length; i++) {
    dotProduct += a[i] * b[i];
  }

  // Vectors are already normalized by Transformers.js (normalize: true)
  // So cosine similarity = dot product
  return dotProduct;
}

/**
 * Finds the best matching category for a video embedding.
 * @param videoEmbedding - Video's embedding vector
 * @param categoryEmbeddings - Map of categoryId to category embedding
 * @returns { categoryId, score } of best match
 */
export function findBestMatch(
  videoEmbedding: Float32Array,
  categoryEmbeddings: Map<number, Float32Array>
): { categoryId: number; score: number } {
  let bestMatch = { categoryId: -1, score: -1 };

  for (const [categoryId, catEmbedding] of categoryEmbeddings) {
    const similarity = cosineSimilarity(videoEmbedding, catEmbedding);
    if (similarity > bestMatch.score) {
      bestMatch = { categoryId, score: similarity };
    }
  }

  return bestMatch;
}
```

**File: src/lib/ml/confidence.ts**
```typescript
import { cosineSimilarity } from './similarity';

export type ConfidenceLevel = 'HIGH' | 'MEDIUM' | 'LOW';

/**
 * Empirical thresholds for all-MiniLM-L6-v2 model.
 * These may need calibration based on actual categorization results.
 * See RESEARCH.md Open Question 3 for details.
 */
export const CONFIDENCE_THRESHOLDS = {
  HIGH: 0.75,    // ≥0.75: Strong semantic match
  MEDIUM: 0.60,  // 0.60-0.74: Moderate match
  // <0.60: Weak match (LOW)
} as const;

/**
 * Determines confidence level based on similarity score.
 * @param score - Cosine similarity score [0, 1]
 * @returns Confidence level classification
 */
export function getConfidenceLevel(score: number): ConfidenceLevel {
  if (score >= CONFIDENCE_THRESHOLDS.HIGH) return 'HIGH';
  if (score >= CONFIDENCE_THRESHOLDS.MEDIUM) return 'MEDIUM';
  return 'LOW';
}

/**
 * Categorizes a video and returns full result with confidence.
 * @param videoEmbedding - Video's embedding vector
 * @param categoryEmbeddings - Map of categoryId to category embedding
 * @returns { categoryId, confidence, score }
 */
export function categorizeWithConfidence(
  videoEmbedding: Float32Array,
  categoryEmbeddings: Map<number, Float32Array>
): { categoryId: number; confidence: ConfidenceLevel; score: number } {
  let bestMatch = { categoryId: -1, score: -1 };

  for (const [categoryId, catEmbedding] of categoryEmbeddings) {
    const similarity = cosineSimilarity(videoEmbedding, catEmbedding);
    if (similarity > bestMatch.score) {
      bestMatch = { categoryId, score: similarity };
    }
  }

  const confidence = getConfidenceLevel(bestMatch.score);

  return { ...bestMatch, confidence };
}
```

**Key details:**
- Thresholds (0.75, 0.60) are empirical starting points from RESEARCH.md
- May need calibration after running on actual data (RESEARCH.md Open Question 3)
- Store model version with confidence logic for future threshold adjustments
- confidence.ts depends on similarity.ts for cosine calculation

**Avoid:**
- Don't re-normalize vectors (already normalized by Transformers.js)
- Don't use L2 distance instead of cosine similarity (embeddings are high-dimensional)
- Don't hard-code thresholds without documenting they're empirical and may need tuning
  </action>
  <verify>
Run: npm run type-check (no TypeScript errors)
Check: File exists at src/lib/ml/similarity.ts with cosineSimilarity and findBestMatch exports
Check: File exists at src/lib/ml/confidence.ts with getConfidenceLevel and categorizeWithConfidence exports
Check: confidence.ts imports from similarity.ts successfully
Check: CONFIDENCE_THRESHOLDS exported as const object
  </verify>
  <done>
Similarity functions implemented with cosineSimilarity and findBestMatch exports, confidence scoring implemented with threshold-based classification (HIGH≥0.75, MEDIUM≥0.60, LOW<0.60), categorizeWithConfidence function combining similarity search and confidence assignment, proper TypeScript types and JSDoc comments included.
  </done>
</task>

</tasks>

<verification>
**Foundation readiness checks:**

1. **IndexedDB Cache Operational:**
   - Open browser DevTools → Application → IndexedDB
   - Database "ml-embeddings" should exist (may need to trigger in Plan 02)
   - Check compound key structure [videoId, modelVersion]

2. **Worker Loads Model:**
   - Worker file can be imported (no syntax errors)
   - Transformers.js dependency installed in node_modules
   - Model ID 'Xenova/all-MiniLM-L6-v2' is correct (verify on Hugging Face Hub)

3. **Similarity Functions Correct:**
   - Unit test: cosineSimilarity([1, 0], [1, 0]) should return 1.0 (identical)
   - Unit test: cosineSimilarity([1, 0], [0, 1]) should return 0.0 (orthogonal)
   - Confidence thresholds: HIGH at 0.75, MEDIUM at 0.60

4. **Type Safety:**
   - Run `npm run type-check` — zero errors
   - All exports match expected signatures from RESEARCH.md patterns
</verification>

<success_criteria>
**Measurable completion:**
- [ ] Four files created: embeddings-cache.ts, worker.ts, similarity.ts, confidence.ts
- [ ] EmbeddingsCache class has get/set/getBatch/setBatch methods
- [ ] Web Worker implements PipelineSingleton with GENERATE_EMBEDDINGS handler
- [ ] cosineSimilarity function calculates dot product for normalized vectors
- [ ] getConfidenceLevel returns HIGH/MEDIUM/LOW based on thresholds
- [ ] @huggingface/transformers installed in package.json
- [ ] Type-check passes with no errors
- [ ] All must_haves key_links present (imports verified)
</success_criteria>

<output>
After completion, create `.planning/phases/05-ml-categorization-engine/05-01-SUMMARY.md`
</output>
