---
phase: 05-ml-categorization-engine
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - src/lib/db/schema.ts
  - src/lib/ml/categorization-engine.ts
autonomous: true

must_haves:
  truths:
    - "System stores ML categorization results in database with confidence scores"
    - "System processes videos in batches of 32 without blocking UI"
    - "System checks cache before generating embeddings to avoid recomputation"
    - "System generates category embeddings from category names"
  artifacts:
    - path: "src/lib/db/schema.ts"
      provides: "mlCategorizations table for storing ML results"
      contains: "export const mlCategorizations"
      min_lines: 20
    - path: "src/lib/ml/categorization-engine.ts"
      provides: "Orchestrator for batch video categorization"
      exports: ["MLCategorizationEngine"]
      min_lines: 150
  key_links:
    - from: "src/lib/ml/categorization-engine.ts"
      to: "src/lib/ml/embeddings-cache.ts"
      via: "EmbeddingsCache instantiation"
      pattern: "new EmbeddingsCache"
    - from: "src/lib/ml/categorization-engine.ts"
      to: "src/lib/ml/worker.ts"
      via: "Web Worker creation"
      pattern: "new Worker.*worker"
    - from: "src/lib/ml/categorization-engine.ts"
      to: "src/lib/ml/confidence.ts"
      via: "categorizeWithConfidence import"
      pattern: "import.*categorizeWithConfidence"
    - from: "src/lib/db/schema.ts"
      to: "videos table"
      via: "foreign key reference"
      pattern: "references.*videos"
---

<objective>
Extend database schema to store ML categorization results and implement the batch categorization orchestrator that coordinates Web Worker embeddings generation, IndexedDB caching, and confidence-based category assignment.

Purpose: Build the core engine that processes 4,000 videos in batches of 32, using cached embeddings where available and generating new ones only when needed. This orchestrates all Phase 5 Plan 01 components into a cohesive categorization system.

Output: Database schema extended with mlCategorizations table and MLCategorizationEngine class ready for server action integration.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-ml-categorization-engine/05-RESEARCH.md

# Plan 01 outputs (dependencies)
@src/lib/ml/embeddings-cache.ts
@src/lib/ml/worker.ts
@src/lib/ml/similarity.ts
@src/lib/ml/confidence.ts

# Schema and types
@src/lib/db/schema.ts
@src/types/categories.ts
@src/types/videos.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend Database Schema for ML Results</name>
  <files>src/lib/db/schema.ts</files>
  <action>
Add mlCategorizations table to schema.ts for storing ML categorization results with confidence scores and model version tracking.

**Add to schema.ts after categoryVideos table:**

```typescript
// --- Phase 5: ML Categorization Engine ---

// Confidence level enum for ML categorization results
export const confidenceLevelEnum = pgEnum('confidence_level', ['HIGH', 'MEDIUM', 'LOW']);

// ML Categorizations table - stores auto-categorization results from Transformers.js
export const mlCategorizations = pgTable('ml_categorizations', {
  id: serial('id').primaryKey(),
  videoId: integer('video_id').references(() => videos.id, { onDelete: 'cascade' }).notNull(),
  suggestedCategoryId: integer('suggested_category_id').references(() => categories.id, { onDelete: 'cascade' }).notNull(),
  confidence: confidenceLevelEnum('confidence').notNull(),
  similarityScore: integer('similarity_score').notNull(), // Store as 0-100 integer (score * 100) for easier display
  modelVersion: text('model_version').notNull().default('all-MiniLM-L6-v2'), // Track model for future upgrades
  createdAt: timestamp('created_at').notNull().defaultNow(),
  acceptedAt: timestamp('accepted_at'), // Set when user accepts the suggestion
  rejectedAt: timestamp('rejected_at'), // Set when user explicitly rejects
  manualCategoryId: integer('manual_category_id').references(() => categories.id), // If user picks different category
});

// Index for efficient querying by video (review interface)
// CREATE INDEX idx_ml_cat_video ON ml_categorizations(video_id);

// Index for filtering by confidence level (low-confidence review workflow)
// CREATE INDEX idx_ml_cat_confidence ON ml_categorizations(confidence, created_at);
```

**Migration note:**
Since this is extending the schema (not modifying existing tables), no data migration needed. New table will be empty initially. Use `drizzle-kit push` in development (per STATE.md decision from 01-01).

**Key details:**
- similarityScore as integer 0-100 for UI display (multiply cosine similarity by 100)
- modelVersion allows tracking model changes for future re-categorization
- acceptedAt/rejectedAt/manualCategoryId support Phase 6 review workflow
- Cascade deletes ensure orphaned records don't persist
- Indexes on videoId and confidence for common query patterns

**Avoid:**
- Don't store Float32Array embeddings in PostgreSQL (use IndexedDB only)
- Don't use ENUM for model version (text allows flexibility for future models)
- Don't forget cascade deletes (videos/categories may be deleted)
  </action>
  <verify>
Run: npm run type-check (no TypeScript errors)
Check: File src/lib/db/schema.ts contains mlCategorizations table export
Check: confidenceLevelEnum includes 'HIGH', 'MEDIUM', 'LOW' values
Check: Foreign key references to videos and categories tables present
Check: Index comments documented for future migration
  </verify>
  <done>
mlCategorizations table added to schema with confidence enum, similarity score as integer, model version tracking, acceptance/rejection timestamps, foreign keys with cascade deletes, and index comments for common queries. Schema ready for Drizzle push in Plan 03.
  </done>
</task>

<task type="auto">
  <name>Task 2: Batch Categorization Engine Orchestrator</name>
  <files>src/lib/ml/categorization-engine.ts</files>
  <action>
Create MLCategorizationEngine class that orchestrates batch video categorization using Web Worker, IndexedDB cache, and confidence scoring, following 05-RESEARCH.md Pattern 2 for batch processing.

**Implementation structure:**

```typescript
import type { Category } from '@/types/categories';
import type { VideoCardData } from '@/types/videos';
import { EmbeddingsCache } from './embeddings-cache';
import { categorizeWithConfidence, type ConfidenceLevel } from './confidence';

const MODEL_VERSION = 'all-MiniLM-L6-v2';
const BATCH_SIZE = 32; // Per RESEARCH.md recommendation

export interface CategorizationResult {
  videoId: number;
  suggestedCategoryId: number;
  confidence: ConfidenceLevel;
  similarityScore: number; // 0-100 for UI display
}

export interface ProgressCallback {
  (current: number, total: number, percentage: number, status: string): void;
}

export class MLCategorizationEngine {
  private worker: Worker | null = null;
  private embeddingsCache: EmbeddingsCache;
  private pendingRequests = new Map<string, { resolve: Function; reject: Function }>();

  constructor() {
    this.embeddingsCache = new EmbeddingsCache();
  }

  /**
   * Initialize Web Worker for embeddings generation.
   * Lazy initialization to avoid loading model until needed.
   */
  private initWorker(): void {
    if (this.worker) return;

    // Create worker from worker.ts file
    this.worker = new Worker(new URL('./worker.ts', import.meta.url), {
      type: 'module',
    });

    this.worker.addEventListener('message', (event: MessageEvent) => {
      const { type, id, embeddings, progress } = event.data;

      if (type === 'EMBEDDINGS_RESULT') {
        const pending = this.pendingRequests.get(id);
        if (pending) {
          pending.resolve(embeddings.map((e: number[]) => new Float32Array(e)));
          this.pendingRequests.delete(id);
        }
      } else if (type === 'LOAD_PROGRESS') {
        console.log('[ML Worker] Model loading progress:', progress);
      }
    });

    this.worker.addEventListener('error', (error) => {
      console.error('[ML Worker] Error:', error);
      // Reject all pending requests
      for (const pending of this.pendingRequests.values()) {
        pending.reject(new Error('Worker error'));
      }
      this.pendingRequests.clear();
    });
  }

  /**
   * Generate embeddings for texts using Web Worker.
   * Returns cached embeddings when available.
   */
  private async generateEmbeddings(texts: string[]): Promise<Float32Array[]> {
    this.initWorker();

    return new Promise((resolve, reject) => {
      const id = Math.random().toString(36).substring(2);
      this.pendingRequests.set(id, { resolve, reject });

      this.worker!.postMessage({
        type: 'GENERATE_EMBEDDINGS',
        texts,
        id,
      });

      // 60 second timeout for batch processing
      setTimeout(() => {
        const pending = this.pendingRequests.get(id);
        if (pending) {
          pending.reject(new Error('Embedding generation timeout'));
          this.pendingRequests.delete(id);
        }
      }, 60000);
    });
  }

  /**
   * Pre-compute category embeddings from category names.
   * Returns Map<categoryId, embedding>.
   */
  async generateCategoryEmbeddings(
    categories: Category[]
  ): Promise<Map<number, Float32Array>> {
    const categoryTexts = categories.map((c) => c.name);
    const embeddings = await this.generateEmbeddings(categoryTexts);

    const categoryEmbeddings = new Map<number, Float32Array>();
    categories.forEach((category, index) => {
      categoryEmbeddings.set(category.id, embeddings[index]);
    });

    return categoryEmbeddings;
  }

  /**
   * Categorize videos in batches with progress updates.
   * Main orchestration method coordinating cache, worker, and similarity.
   */
  async categorizeVideos(
    videos: VideoCardData[],
    categories: Category[],
    onProgress?: ProgressCallback
  ): Promise<CategorizationResult[]> {
    const results: CategorizationResult[] = [];

    // Step 1: Pre-compute category embeddings (reused for all videos)
    onProgress?.(0, videos.length, 0, 'Preparing categories...');
    const categoryEmbeddings = await this.generateCategoryEmbeddings(categories);

    // Step 2: Process videos in batches
    for (let i = 0; i < videos.length; i += BATCH_SIZE) {
      const batch = videos.slice(i, i + BATCH_SIZE);
      const batchNum = Math.floor(i / BATCH_SIZE) + 1;
      const totalBatches = Math.ceil(videos.length / BATCH_SIZE);

      onProgress?.(
        i,
        videos.length,
        Math.round((i / videos.length) * 100),
        `Processing batch ${batchNum}/${totalBatches}...`
      );

      // Step 2a: Check cache for existing embeddings
      const cachedEmbeddings = await this.embeddingsCache.getBatch(
        batch.map((v) => v.id),
        MODEL_VERSION
      );

      // Step 2b: Generate embeddings for uncached videos
      const uncachedVideos = batch.filter((v) => !cachedEmbeddings.has(v.id));
      if (uncachedVideos.length > 0) {
        const texts = uncachedVideos.map(
          (v) => `${v.title} ${v.channelTitle || ''}`
        );
        const newEmbeddings = await this.generateEmbeddings(texts);

        // Cache new embeddings
        await this.embeddingsCache.setBatch(
          uncachedVideos.map((v, idx) => ({
            videoId: v.id,
            embedding: newEmbeddings[idx],
          })),
          MODEL_VERSION
        );

        // Merge with cached
        uncachedVideos.forEach((v, idx) => {
          cachedEmbeddings.set(v.id, newEmbeddings[idx]);
        });
      }

      // Step 2c: Categorize each video in batch
      for (const video of batch) {
        const videoEmbedding = cachedEmbeddings.get(video.id)!;
        const match = categorizeWithConfidence(videoEmbedding, categoryEmbeddings);

        results.push({
          videoId: video.id,
          suggestedCategoryId: match.categoryId,
          confidence: match.confidence,
          similarityScore: Math.round(match.score * 100), // Convert to 0-100
        });
      }
    }

    // Final progress update
    onProgress?.(videos.length, videos.length, 100, 'Categorization complete');

    return results;
  }

  /**
   * Terminate worker and cleanup resources.
   * Call when categorization is complete.
   */
  terminate(): void {
    if (this.worker) {
      this.worker.terminate();
      this.worker = null;
    }
    this.pendingRequests.clear();
  }
}
```

**Key details:**
- Lazy worker initialization (model loads only when categorizeVideos called)
- Cache-first strategy: check IndexedDB before generating embeddings
- Batch size 32 per RESEARCH.md (hardware-independent conservative value)
- Progress callback with status messages for UI feedback
- Category embeddings pre-computed once, reused for all videos
- Timeout handling for worker requests (60s batch processing limit)
- Similarity score scaled to 0-100 integer for database/UI display

**Avoid:**
- Don't process all 4,000 videos in single batch (browser freezes)
- Don't regenerate category embeddings per video (expensive waste)
- Don't forget to terminate worker when done (memory leak per RESEARCH.md Pitfall 4)
- Don't block main thread (all heavy work in Web Worker)
  </action>
  <verify>
Run: npm run type-check (no TypeScript errors)
Check: File exists at src/lib/ml/categorization-engine.ts
Check: MLCategorizationEngine class exported with categorizeVideos method
Check: Code imports from embeddings-cache, confidence, worker modules
Check: BATCH_SIZE constant set to 32
Check: ProgressCallback type defined with current/total/percentage/status parameters
Check: terminate() method exists for cleanup
  </verify>
  <done>
MLCategorizationEngine implemented with batch processing (32 videos per batch), cache-first embedding strategy, pre-computed category embeddings, progress callbacks for UI updates, worker lifecycle management with timeout handling, and similarity score scaling to 0-100 integer. Ready for server action integration.
  </done>
</task>

</tasks>

<verification>
**Integration readiness checks:**

1. **Schema Extension Valid:**
   - Run `npm run type-check` â€” no errors
   - mlCategorizations table has all required fields
   - Enum confidenceLevelEnum matches ConfidenceLevel type from confidence.ts

2. **Engine Orchestrates Components:**
   - MLCategorizationEngine imports EmbeddingsCache successfully
   - Worker creation uses correct path (./worker.ts)
   - categorizeWithConfidence imported from confidence.ts
   - All Plan 01 components wired together

3. **Batch Processing Logic:**
   - BATCH_SIZE = 32 (per RESEARCH.md)
   - Cache checked before generating embeddings (getBatch call)
   - Progress callback invoked per batch with status
   - Category embeddings computed once, reused

4. **Memory Management:**
   - terminate() method exists for worker cleanup
   - Pending requests tracked in Map
   - Timeout prevents indefinite hangs
</verification>

<success_criteria>
**Measurable completion:**
- [ ] mlCategorizations table added to schema.ts with 10 fields
- [ ] confidenceLevelEnum includes HIGH, MEDIUM, LOW values
- [ ] MLCategorizationEngine class with categorizeVideos method implemented
- [ ] Batch size set to 32 videos
- [ ] Cache-first strategy: getBatch before generateEmbeddings
- [ ] Progress callback invoked with current/total/percentage/status
- [ ] Worker lifecycle: initWorker, terminate methods present
- [ ] Type-check passes with no errors
- [ ] All must_haves key_links present (imports verified)
</success_criteria>

<output>
After completion, create `.planning/phases/05-ml-categorization-engine/05-02-SUMMARY.md`
</output>
