---
phase: 13-watch-later-csv-import-metadata-enrichment
plan: 02
type: execute
wave: 2
depends_on: ["13-01"]
files_modified:
  - src/app/actions/import.ts
autonomous: true

must_haves:
  truths:
    - "A batch of up to 50 video IDs is enriched with metadata from the YouTube API in a single server action call"
    - "Videos already in the database with valid metadata are skipped (no redundant API calls)"
    - "Deleted/private videos not returned by the API are stored as placeholder records with title '[Unavailable Video]'"
    - "Quota usage is tracked for each API call via the existing quota tracking system"
    - "Each batch returns counts of processed, unavailable, and skipped videos"
  artifacts:
    - path: "src/app/actions/import.ts"
      provides: "importMetadataBatch server action"
      exports: ["parseAndInitialiseImport", "importMetadataBatch"]
  key_links:
    - from: "src/app/actions/import.ts"
      to: "src/lib/youtube/videos.ts"
      via: "import fetchVideoBatch"
      pattern: "import.*fetchVideoBatch.*youtube/videos"
    - from: "src/app/actions/import.ts"
      to: "src/lib/db/schema.ts"
      via: "Drizzle insert for unavailable video placeholders"
      pattern: "insert\\(videos\\).*Unavailable Video"
---

<objective>
Add a batch metadata enrichment server action that fetches video details from the YouTube API for CSV-imported video IDs.

Purpose: After CSV parsing (13-01), video IDs exist as strings but have no metadata. This plan enriches them by calling the YouTube API in batches of 50, reusing the existing `fetchVideoBatch()` function. It also handles unavailable/deleted videos by creating placeholder records so foreign key constraints are satisfied when creating playlist-video relationships (13-03).

Output: A new `importMetadataBatch` export added to the existing `src/app/actions/import.ts` file.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/phases/13-watch-later-csv-import-metadata-enrichment/13-RESEARCH.md

Key codebase references:
@src/lib/youtube/videos.ts — fetchVideoBatch function (lines 241-325): takes accessToken + videoIds, calls youtube.videos.list in batches of 50, upserts to videos table, tracks quota. Returns youtube_v3.Schema$Video[].
@src/lib/db/schema.ts — videos table (youtubeId unique, title notNull)
@src/lib/youtube/quota.ts — trackQuotaUsage utility (already called inside fetchVideoBatch)
@src/app/actions/import.ts — file from 13-01 (parseAndInitialiseImport already exists here)

CRITICAL from RESEARCH: fetchVideoBatch already handles batching internally (splits into groups of 50), BUT for the import flow we want to process ONE batch of 50 per server action invocation so the client can update progress between batches. Do NOT pass all 3,932 IDs to fetchVideoBatch in one call — that would be a single long-running action that times out. Instead, slice 50 IDs per call and let the client drive the loop.

CRITICAL from MEMORY.md: PostgreSQL bigint returns strings via Drizzle — wrap any COUNT results with Number().
</context>

<tasks>

<task type="auto">
  <name>Task 1: Batch metadata enrichment server action with unavailable video handling</name>
  <files>
    src/app/actions/import.ts
  </files>
  <action>
    Add to the existing `src/app/actions/import.ts` file (which already has parseAndInitialiseImport from Plan 13-01).

    Add imports: `db` from `@/lib/db`, `videos` from `@/lib/db/schema`, `fetchVideoBatch` from `@/lib/youtube/videos`, `inArray` from `drizzle-orm`.

    Export async function `importMetadataBatch(videoIds: string[], startIndex: number, batchSize: number = 50)` returning:
    ```typescript
    Promise<{
      success: boolean;
      processed: number;
      unavailable: number;
      skipped: number;
      error?: string;
    }>
    ```

    Implementation:
    1. Auth check: `const session = await auth()`. If no `session?.access_token`, return `{ success: false, processed: 0, unavailable: 0, skipped: 0, error: 'Not authenticated' }`.

    2. Slice the batch: `const batch = videoIds.slice(startIndex, startIndex + batchSize)`. If batch is empty, return success with all zeroes.

    3. Check which videos already exist with valid metadata (re-import quota conservation):
       ```typescript
       const existing = await db
         .select({ youtubeId: videos.youtubeId })
         .from(videos)
         .where(inArray(videos.youtubeId, batch));
       const existingIds = new Set(existing.map(v => v.youtubeId));
       const newIds = batch.filter(id => !existingIds.has(id));
       const skipped = batch.length - newIds.length;
       ```

    4. If `newIds.length > 0`:
       - Call `fetchVideoBatch(session.access_token, newIds)` — this calls the YouTube API, upserts to DB, and tracks quota. Note: fetchVideoBatch internally batches by 50, but since we already sliced to 50, it will make exactly 1 API call.
       - Detect unavailable videos: compare sent IDs vs returned IDs.
         ```typescript
         const returned = await fetchVideoBatch(session.access_token, newIds);
         const returnedIds = new Set(returned.map(v => v.id).filter((id): id is string => !!id));
         const missingIds = newIds.filter(id => !returnedIds.has(id));
         ```
       - Insert placeholder records for each missing/unavailable video:
         ```typescript
         for (const id of missingIds) {
           await db.insert(videos).values({
             youtubeId: id,
             title: '[Unavailable Video]',
             lastFetched: new Date(),
             createdAt: new Date(),
             updatedAt: new Date(),
           }).onConflictDoNothing();
         }
         ```
       - `unavailable = missingIds.length`

    5. Return `{ success: true, processed: newIds.length - unavailable, unavailable, skipped }`.

    6. Wrap all DB/API operations in try/catch. On error, return `{ success: false, processed: 0, unavailable: 0, skipped: 0, error: message }`.

    IMPORTANT: The function signature takes ALL video IDs and a startIndex so the client does not need to slice — it just increments startIndex by batchSize each iteration. This matches the polling pattern from the sync page where the client drives the loop.
  </action>
  <verify>
    `npx tsc --noEmit` passes.
    importMetadataBatch is exported from src/app/actions/import.ts alongside parseAndInitialiseImport.
    The function: (a) checks auth, (b) skips existing videos, (c) calls fetchVideoBatch for new ones, (d) inserts placeholders for unavailable, (e) returns structured counts.
  </verify>
  <done>
    importMetadataBatch processes a single batch of up to 50 videos per invocation. It skips videos already in the DB (quota conservation for re-import), fetches metadata for new videos via the existing fetchVideoBatch function, inserts placeholder records for deleted/private videos, and returns counts of processed/unavailable/skipped. The client drives the loop by incrementing startIndex.
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` — no type errors
2. importMetadataBatch correctly slices batch from startIndex
3. Existing videos are detected and skipped (no redundant API calls)
4. Unavailable videos get placeholder records with `title: '[Unavailable Video]'`
5. fetchVideoBatch is reused (not hand-rolled API calls)
6. Structured error returns — never throws
</verification>

<success_criteria>
- Single batch of 50 video IDs is processed per server action invocation
- Videos already in DB are skipped (WL-09 re-import quota conservation)
- Deleted/private videos get placeholder DB records (WL-05)
- Quota is tracked via existing fetchVideoBatch internals
- Returns {processed, unavailable, skipped} counts for progress UI
</success_criteria>

<output>
After completion, create `.planning/phases/13-watch-later-csv-import-metadata-enrichment/13-02-SUMMARY.md`
</output>
